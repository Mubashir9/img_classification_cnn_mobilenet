{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddd8f3a7-ccc1-42af-a8a3-c80f8f4f89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af7d58f4-0edd-411b-88d5-416d4b9a399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4beabb14-0c2c-4336-813c-ced95eaaf948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"The device is\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b964f-87fb-4caa-a0a5-afbd935c1393",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d01090-962e-453f-90cf-0e7ee88e6fe4",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45641af4-e30f-4b20-8ae8-e43aadf86111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7605efac-be98-46b4-b1ff-d32d22116daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a65fe642-8c64-4dc4-9cf9-9c65caf35e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training set size: 50000\n",
      "Test set size: 10000\n",
      "Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(f'Full training set size: {len(full_trainset)}')\n",
    "print(f'Test set size: {len(testset)}')\n",
    "print(f'Classes: {full_trainset.classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "396c78d5-11cc-4dce-8f5f-1223faf8d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Balanced Subset (1000 images per class)\n",
    "\n",
    "def create_balanced_subset(dataset, samples_per_class=1000, random_seed=42):\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Get all data and targets\n",
    "    data = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        img, label = dataset[i]\n",
    "        data.append(img)\n",
    "        targets.append(label)\n",
    "    \n",
    "    data = torch.stack(data)\n",
    "    targets = torch.tensor(targets)\n",
    "    \n",
    "    # Create balanced subset\n",
    "    subset_data = []\n",
    "    subset_targets = []\n",
    "    \n",
    "    num_classes = len(dataset.classes)\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Find all indices for this class\n",
    "        class_indices = torch.where(targets == class_idx)[0]\n",
    "        \n",
    "        # Randomly select samples_per_class indices\n",
    "        selected_indices = torch.randperm(len(class_indices))[:samples_per_class]\n",
    "        selected_indices = class_indices[selected_indices]\n",
    "        \n",
    "        # Add selected samples to subset\n",
    "        subset_data.append(data[selected_indices])\n",
    "        subset_targets.extend([class_idx] * samples_per_class)\n",
    "    \n",
    "    # Combine all classes\n",
    "    subset_data = torch.cat(subset_data, dim=0)\n",
    "    subset_targets = torch.tensor(subset_targets)\n",
    "    \n",
    "    return subset_data, subset_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ab404ba-9d96-409c-9102-debb8bf31b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_targets = create_balanced_subset(full_trainset, samples_per_class=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4d22a80-27a9-4448-82ef-2205358c0473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training subset size: torch.Size([10000, 3, 32, 32])\n",
      "Test set size: torch.Size([10000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Extract test data\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for i in range(len(testset)):\n",
    "    img, label = testset[i]\n",
    "    test_data.append(img)\n",
    "    test_labels.append(label)\n",
    "\n",
    "test_images = torch.stack(test_data)\n",
    "test_targets = torch.tensor(test_labels)\n",
    "\n",
    "print(f'Training subset size: {train_images.shape}')\n",
    "print(f'Test set size: {test_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bb347ef-39a7-4b39-8155-01b6cba86abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training subset:\n",
      "  Class 0 (airplane): 1000 samples\n",
      "  Class 1 (automobile): 1000 samples\n",
      "  Class 2 (bird): 1000 samples\n",
      "  Class 3 (cat): 1000 samples\n",
      "  Class 4 (deer): 1000 samples\n",
      "  Class 5 (dog): 1000 samples\n",
      "  Class 6 (frog): 1000 samples\n",
      "  Class 7 (horse): 1000 samples\n",
      "  Class 8 (ship): 1000 samples\n",
      "  Class 9 (truck): 1000 samples\n"
     ]
    }
   ],
   "source": [
    "# Verify class distribution\n",
    "unique_classes, counts = torch.unique(train_targets, return_counts=True)\n",
    "print(f'Class distribution in training subset:')\n",
    "for class_idx, count in zip(unique_classes, counts):\n",
    "    print(f'  Class {class_idx} ({full_trainset.classes[class_idx]}): {count} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bab0a8-ca8f-4ff1-a0db-36633f8fb7e5",
   "metadata": {},
   "source": [
    "## Defining the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5aa425b7-f6ad-4ace-a776-a36e0209e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        # Data is already normalized and in tensor format\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        x, y = self.x[ix], self.y[ix]\n",
    "        return x.to(device), y.to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36725a15-1a28-44e2-bc08-6ba747aefcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data():\n",
    "    train = CIFAR10Dataset(train_images, train_targets)\n",
    "    trn_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    return trn_dl\n",
    "\n",
    "def get_test_data():\n",
    "    test = CIFAR10Dataset(test_images, test_targets)\n",
    "    test_dl = DataLoader(test, batch_size=32, shuffle=False)\n",
    "    return test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9fbb6d-24b3-49d4-9807-93ef640ab5f7",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d2a14-38cb-4c7c-a48e-5592f62ac6a8",
   "metadata": {},
   "source": [
    "#### Define Custom Neural Network, Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51cf9cad-4c51-4a4e-9854-160a2a90a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn():\n",
    "    model = nn.Sequential(\n",
    "        # First convolutional block\n",
    "        nn.Conv2d(3, 32, kernel_size=3, padding=1),  # 32x32x32\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, kernel_size=3, padding=1), # 32x32x32\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),       # 16x16x32\n",
    "        nn.Dropout2d(0.25),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1), # 16x16x64\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, padding=1), # 16x16x64\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),       # 8x8x64\n",
    "        nn.Dropout2d(0.25),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1), # 8x8x128\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(128, 128, kernel_size=3, padding=1), # 8x8x128\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),        # 4x4x128\n",
    "        nn.Dropout2d(0.25),\n",
    "        \n",
    "        # Fully connected layers\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(128 * 4 * 4, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, 10)\n",
    "    ).to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0bc8d7-4b81-4f5b-ab19-75b65d5271a7",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7553dc-e8ea-4640-9a8c-e04e24e84bb3",
   "metadata": {},
   "source": [
    "#### Load and Adapt ModelNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6b77d0b-9689-459c-8241-50baaf0c0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mobilenetv2():\n",
    "    # Load pretrained MobileNetV2\n",
    "    model = models.mobilenet_v2(pretrained=True)\n",
    "    \n",
    "    # Modify the classifier for CIFAR-10 (10 classes)\n",
    "    # MobileNetV2's classifier is a single linear layer\n",
    "    num_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(num_features, 10)\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Properly initialize the new classifier layer\n",
    "    nn.init.xavier_uniform_(model.classifier[1].weight)\n",
    "    nn.init.zeros_(model.classifier[1].bias)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dff3b6-3727-4c33-90d9-6c01f225f13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f77e471-6c0f-41fb-8ff3-39180552b541",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "873917ba-9eb9-4a75-aead-822d0915e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Function\n",
    "def train_batch(x, y, model, optimizer, loss_fn):\n",
    "    model.train()  # Set the model to the \"train\" mode\n",
    "    \n",
    "    # Apply the model to the inputs\n",
    "    prediction = model(x)\n",
    "    \n",
    "    # Compute loss\n",
    "    batch_loss = loss_fn(prediction, y)\n",
    "    \n",
    "    # Based on the forward pass in `model(x)`, compute all the\n",
    "    # gradients of 'model.parameters()'\n",
    "    batch_loss.backward()\n",
    "    \n",
    "    # Apply new-weights = f(old-weights, old-weight-gradients)\n",
    "    # where \"f\" is the optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Flush gradients memory for next batch of calculations\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ff1d458-7b79-4eae-a88d-c3f74df1b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y, model):\n",
    "    model.eval()  # Set the model to the \"evaluation\" mode\n",
    "    \n",
    "    # Get the prediction matrix for a tensor of `x` images\n",
    "    prediction = model(x)\n",
    "    \n",
    "    # Compute if the location of maximum in each row\n",
    "    # coincides with ground truth\n",
    "    max_values, argmaxes = prediction.max(-1)\n",
    "    \n",
    "    # Compute the accuracy by checking if the locations of maximum values\n",
    "    # coincide with the ground truth values `y`\n",
    "    is_correct = argmaxes == y\n",
    "    return is_correct.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aad21a22-78b3-404a-b74f-842601f0df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss_fn, train_dl, num_epochs=10, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Modular training function that works for either model\n",
    "    Uses the same hyperparameters for both models\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Initialize the lists that will contain the accuracy and loss values\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        \n",
    "        # Initialize the lists for each batch\n",
    "        epoch_accuracies = []\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Iterate for each batch\n",
    "        for batch in train_dl:\n",
    "            x, y = batch\n",
    "            # Compute the loss and accuracy\n",
    "            batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
    "            batch_accuracy = accuracy(x, y, model)\n",
    "            epoch_losses.append(batch_loss)\n",
    "            epoch_accuracies.extend(batch_accuracy)\n",
    "        \n",
    "        # Store the mean loss and accuracy within an epoch\n",
    "        epoch_loss = np.mean(epoch_losses)\n",
    "        epoch_accuracy = np.mean(epoch_accuracies)\n",
    "        losses.append(epoch_loss)\n",
    "        accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        print(f\"  Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}\")\n",
    "    \n",
    "    return model, accuracies, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125ba8d-0446-4634-b2e7-38cebf92ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING CUSTOM CNN\n",
      "==================================================\n",
      "\n",
      "Training Custom CNN...\n",
      "Epoch: 0\n",
      "  Train Loss: 1.8174, Train Accuracy: 0.3818\n",
      "Epoch: 1\n",
      "  Train Loss: 1.5345, Train Accuracy: 0.5162\n",
      "Epoch: 2\n",
      "  Train Loss: 1.3877, Train Accuracy: 0.5761\n",
      "Epoch: 3\n",
      "  Train Loss: 1.2546, Train Accuracy: 0.6375\n",
      "Epoch: 4\n",
      "  Train Loss: 1.1472, Train Accuracy: 0.6810\n",
      "Epoch: 5\n",
      "  Train Loss: 1.0614, Train Accuracy: 0.7170\n",
      "Epoch: 6\n",
      "  Train Loss: 0.9934, Train Accuracy: 0.7486\n",
      "Epoch: 7\n",
      "  Train Loss: 0.9263, Train Accuracy: 0.7643\n",
      "Epoch: 8\n",
      "  Train Loss: 0.8671, Train Accuracy: 0.7949\n",
      "Epoch: 9\n",
      "  Train Loss: 0.8253, Train Accuracy: 0.8195\n",
      "\n",
      "==================================================\n",
      "TRAINING MOBILENETV2\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\my_pytorch_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\envs\\my_pytorch_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\Admin/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MobileNetV2...\n",
      "Epoch: 0\n",
      "  Train Loss: 1.6348, Train Accuracy: 0.5463\n",
      "Epoch: 1\n",
      "  Train Loss: 1.1827, Train Accuracy: 0.6850\n",
      "Epoch: 2\n"
     ]
    }
   ],
   "source": [
    "# Get data loaders\n",
    "train_dl = get_training_data()\n",
    "test_dl = get_test_data()\n",
    "\n",
    "# Training parameters (same hyperparameters for both models)\n",
    "num_epochs = 10\n",
    "\n",
    "# Train both models on the same training subset\n",
    "print(\"=\"*50)\n",
    "print(\"TRAINING CUSTOM CNN\")\n",
    "print(\"=\"*50)\n",
    "custom_model, custom_loss_fn, custom_optimizer = get_cnn()\n",
    "custom_model, custom_accuracies, custom_losses = train_model(\n",
    "    custom_model, custom_optimizer, custom_loss_fn, train_dl, num_epochs, \"Custom CNN\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14eb62-6649-4455-8922-1add12916b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING MOBILENETV2\")\n",
    "print(\"=\"*50)\n",
    "mobilenet_model, mobilenet_loss_fn, mobilenet_optimizer = get_mobilenetv2()\n",
    "mobilenet_model, mobilenet_accuracies, mobilenet_losses = train_model(\n",
    "    mobilenet_model, mobilenet_optimizer, mobilenet_loss_fn, train_dl, num_epochs, \"MobileNetV2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc1af4-3815-4e91-8aba-6127d044800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the Model\n",
    "\n",
    "test_dl = get_test_data()\n",
    "test_accuracies = []\n",
    "test_losses = []\n",
    "\n",
    "for batch in test_dl:\n",
    "    x, y = batch\n",
    "    batch_accuracy = accuracy(x, y, model)\n",
    "    test_accuracies.extend(batch_accuracy)\n",
    "\n",
    "test_accuracy = np.mean(test_accuracies)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "print(f'Final Train Accuracy: {accuracies[-1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53194694-916d-41bd-947c-ff4721544430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress comparison\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(num_epochs), custom_losses, label='Custom CNN', marker='o')\n",
    "plt.plot(range(num_epochs), mobilenet_losses, label='MobileNetV2', marker='s')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(num_epochs), custom_accuracies, label='Custom CNN', marker='o')\n",
    "plt.plot(range(num_epochs), mobilenet_accuracies, label='MobileNetV2', marker='s')\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8abc3-924d-4feb-92e7-d93e0b2bc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51795d9-f05a-4e35-ba04-24a3661ea321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dl, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate a model on the full CIFAR-10 test set\n",
    "    Report overall test accuracy clearly\n",
    "    \"\"\"\n",
    "    test_accuracies = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for batch in test_dl:\n",
    "        x, y = batch\n",
    "        batch_accuracy = accuracy(x, y, model)\n",
    "        test_accuracies.extend(batch_accuracy)\n",
    "        \n",
    "        # Get predictions for confusion matrix\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = np.mean(test_accuracies)\n",
    "    print(f'{model_name} Test Accuracy: {test_accuracy:.4f}')\n",
    "    \n",
    "    return test_accuracy, all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91cc32f-1101-4ef4-be10-23ac23154861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on the full CIFAR-10 test set\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "custom_test_acc, custom_preds, custom_targets = evaluate_model(\n",
    "    custom_model, test_dl, \"Custom CNN\"\n",
    ")\n",
    "\n",
    "mobilenet_test_acc, mobilenet_preds, mobilenet_targets = evaluate_model(\n",
    "    mobilenet_model, test_dl, \"MobileNetV2\"\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Custom CNN Train Accuracy: {custom_accuracies[-1]:.4f}\")\n",
    "print(f\"Final MobileNetV2 Train Accuracy: {mobilenet_accuracies[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb15afd-d209-49fa-b12c-27121aa6e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141071c3-d3f9-4e05-95ef-a0a28d5bafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, model_name):\n",
    "    \"\"\"\n",
    "    Generate and display confusion matrices using sklearn\n",
    "    Ensure axes are labeled and readable\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    print(f\"\\n{model_name} - Per-class accuracies:\")\n",
    "    for i, class_name in enumerate(classes):\n",
    "        print(f\"  {class_name}: {per_class_acc[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ee523-9b14-4b12-9473-507379c9c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display confusion matrices for both models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "plot_confusion_matrix(custom_targets, custom_preds, full_trainset.classes, \"Custom CNN\")\n",
    "plot_confusion_matrix(mobilenet_targets, mobilenet_preds, full_trainset.classes, \"MobileNetV2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ccf505-ab8c-4071-a8b0-643dbd9517c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final Results Summary\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Custom CNN:\")\n",
    "print(f\"  - Parameters: {count_parameters(custom_model):,}\")\n",
    "print(f\"  - Final Train Accuracy: {custom_accuracies[-1]:.4f}\")\n",
    "print(f\"  - Test Accuracy: {custom_test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nMobileNetV2:\")\n",
    "print(f\"  - Parameters: {count_parameters(mobilenet_model):,}\")\n",
    "print(f\"  - Final Train Accuracy: {mobilenet_accuracies[-1]:.4f}\")\n",
    "print(f\"  - Test Accuracy: {mobilenet_test_acc:.4f}\")\n",
    "\n",
    "accuracy_diff = mobilenet_test_acc - custom_test_acc\n",
    "print(f\"\\nMobileNetV2 advantage: {accuracy_diff:+.4f} ({accuracy_diff*100:+.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176827c1-5402-4b4b-a575-66d7686f8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4997dcc-943a-4e29-a339-f50bf404c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 8: PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare test accuracy\n",
    "print(\"1. TEST ACCURACY COMPARISON:\")\n",
    "print(f\"   Custom CNN Test Accuracy: {custom_test_acc:.4f} ({custom_test_acc*100:.2f}%)\")\n",
    "print(f\"   MobileNetV2 Test Accuracy: {mobilenet_test_acc:.4f} ({mobilenet_test_acc*100:.2f}%)\")\n",
    "print(f\"   Difference: {accuracy_diff:+.4f} ({accuracy_diff*100:+.2f}%)\")\n",
    "\n",
    "# Analyze training stability and convergence\n",
    "print(\"\\n2. TRAINING STABILITY AND CONVERGENCE:\")\n",
    "custom_loss_std = np.std(custom_losses)\n",
    "mobilenet_loss_std = np.std(mobilenet_losses)\n",
    "custom_acc_improvement = custom_accuracies[-1] - custom_accuracies[0]\n",
    "mobilenet_acc_improvement = mobilenet_accuracies[-1] - mobilenet_accuracies[0]\n",
    "\n",
    "print(f\"   Custom CNN:\")\n",
    "print(f\"     - Loss standard deviation: {custom_loss_std:.4f}\")\n",
    "print(f\"     - Accuracy improvement: {custom_acc_improvement:.4f}\")\n",
    "print(f\"     - Final loss: {custom_losses[-1]:.4f}\")\n",
    "print(f\"   MobileNetV2:\")\n",
    "print(f\"     - Loss standard deviation: {mobilenet_loss_std:.4f}\")\n",
    "print(f\"     - Accuracy improvement: {mobilenet_acc_improvement:.4f}\")\n",
    "print(f\"     - Final loss: {mobilenet_losses[-1]:.4f}\")\n",
    "\n",
    "# Analyze generalization\n",
    "print(\"\\n3. GENERALIZATION TO UNSEEN DATA:\")\n",
    "custom_overfitting = custom_accuracies[-1] - custom_test_acc\n",
    "mobilenet_overfitting = mobilenet_accuracies[-1] - mobilenet_test_acc\n",
    "\n",
    "print(f\"   Custom CNN:\")\n",
    "print(f\"     - Train-Test gap: {custom_overfitting:+.4f}\")\n",
    "print(f\"     - Generalization: {'Good' if abs(custom_overfitting) < 0.05 else 'Moderate' if abs(custom_overfitting) < 0.1 else 'Poor'}\")\n",
    "print(f\"   MobileNetV2:\")\n",
    "print(f\"     - Train-Test gap: {mobilenet_overfitting:+.4f}\")\n",
    "print(f\"     - Generalization: {'Good' if abs(mobilenet_overfitting) < 0.05 else 'Moderate' if abs(mobilenet_overfitting) < 0.1 else 'Poor'}\")\n",
    "\n",
    "# Discuss trade-offs\n",
    "print(\"\\n4. TRADE-OFFS (COMPLEXITY VS. PERFORMANCE):\")\n",
    "param_ratio = count_parameters(mobilenet_model) / count_parameters(custom_model)\n",
    "print(f\"   Parameter ratio (MobileNet/Custom): {param_ratio:.1f}x\")\n",
    "print(f\"   Custom CNN: Simpler architecture, fewer parameters, faster training\")\n",
    "print(f\"   MobileNetV2: Pretrained features, more complex, better performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76771424-4ab8-4dc1-9937-cae9649146f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d3c9a-237f-4907-8336-7dd6ef591166",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 9: MISCLASSIFIED CASE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def analyze_misclassifications(y_true, y_pred, test_images, classes, model_name, num_samples=8):\n",
    "    \"\"\"\n",
    "    Identify and visualize misclassified test samples\n",
    "    Analyze why these images may have been incorrectly classified\n",
    "    \"\"\"\n",
    "    # Find misclassified indices\n",
    "    misclassified_indices = []\n",
    "    for i, (true_label, pred_label) in enumerate(zip(y_true, y_pred)):\n",
    "        if true_label != pred_label:\n",
    "            misclassified_indices.append(i)\n",
    "    \n",
    "    print(f\"\\n{model_name} Misclassification Analysis:\")\n",
    "    print(f\"Total misclassified: {len(misclassified_indices)} out of {len(y_true)}\")\n",
    "    print(f\"Error rate: {len(misclassified_indices)/len(y_true)*100:.2f}%\")\n",
    "    \n",
    "    # Show sample misclassifications\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sample_indices = np.random.choice(misclassified_indices, min(num_samples, len(misclassified_indices)), replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        # Convert from normalized tensor to displayable image\n",
    "        img = test_images[idx].permute(1, 2, 0)  # Change from CHW to HWC\n",
    "        img = img * 0.5 + 0.5  # Denormalize from [-1,1] to [0,1]\n",
    "        img = torch.clamp(img, 0, 1)  # Ensure values are in [0,1]\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        true_label = classes[y_true[idx]]\n",
    "        pred_label = classes[y_pred[idx]]\n",
    "        plt.title(f'True: {true_label}\\nPred: {pred_label}', color='red')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Misclassified Samples', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze confusion patterns\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\nMost confused class pairs for {model_name}:\")\n",
    "    \n",
    "    # Find top confusion pairs (excluding diagonal)\n",
    "    confusion_pairs = []\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            if i != j and cm[i][j] > 0:\n",
    "                confusion_pairs.append((i, j, cm[i][j]))\n",
    "    \n",
    "    # Sort by confusion count\n",
    "    confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    for i, (true_class, pred_class, count) in enumerate(confusion_pairs[:5]):\n",
    "        print(f\"  {i+1}. {classes[true_class]} → {classes[pred_class]}: {count} cases\")\n",
    "\n",
    "# Analyze misclassifications for both models\n",
    "analyze_misclassifications(custom_targets, custom_preds, test_images, full_trainset.classes, \"Custom CNN\")\n",
    "analyze_misclassifications(mobilenet_targets, mobilenet_preds, test_images, full_trainset.classes, \"MobileNetV2\")\n",
    "\n",
    "print(\"\\nVISUAL SIMILARITY ANALYSIS:\")\n",
    "print(\"Common confusion patterns often occur between visually similar classes:\")\n",
    "print(\"- Cat ↔ Dog: Similar fur textures and poses\")\n",
    "print(\"- Automobile ↔ Truck: Both are vehicles with similar shapes\")\n",
    "print(\"- Bird ↔ Airplane: Both can appear in sky with similar silhouettes\")\n",
    "print(\"- Deer ↔ Horse: Similar animal body structures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a78c3-2f83-4bb3-9428-8880bee6c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563529e6-a053-4d05-9f9c-c6f72828c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 10: EFFICIENCY COMMENTARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Model size comparison\n",
    "custom_params = count_parameters(custom_model)\n",
    "mobilenet_params = count_parameters(mobilenet_model)\n",
    "\n",
    "print(\"1. MODEL SIZE (Number of Parameters):\")\n",
    "print(f\"   Custom CNN: {custom_params:,} parameters\")\n",
    "print(f\"   MobileNetV2: {mobilenet_params:,} parameters\")\n",
    "print(f\"   Size ratio: {mobilenet_params/custom_params:.1f}x larger\")\n",
    "\n",
    "# Inference speed analysis\n",
    "print(\"\\n2. INFERENCE SPEED:\")\n",
    "print(\"   Custom CNN:\")\n",
    "print(\"     - Smaller model = faster inference\")\n",
    "print(\"     - Simple architecture = less computational overhead\")\n",
    "print(\"     - Suitable for real-time applications\")\n",
    "print(\"   MobileNetV2:\")\n",
    "print(\"     - Larger model = slower inference\")\n",
    "print(\"     - More complex operations (depthwise separable convolutions)\")\n",
    "print(\"     - Still relatively efficient compared to other pretrained models\")\n",
    "\n",
    "# Measure actual inference time\n",
    "import time\n",
    "\n",
    "def measure_inference_time(model, test_dl, num_batches=10):\n",
    "    \"\"\"Measure average inference time per batch\"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_dl):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            x, y = batch\n",
    "            \n",
    "            start_time = time.time()\n",
    "            _ = model(x)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "    \n",
    "    return np.mean(times)\n",
    "\n",
    "custom_inference_time = measure_inference_time(custom_model, test_dl)\n",
    "mobilenet_inference_time = measure_inference_time(mobilenet_model, test_dl)\n",
    "\n",
    "print(f\"\\n   Measured inference times (per batch of 32 images):\")\n",
    "print(f\"   Custom CNN: {custom_inference_time*1000:.2f} ms\")\n",
    "print(f\"   MobileNetV2: {mobilenet_inference_time*1000:.2f} ms\")\n",
    "print(f\"   Speed ratio: {mobilenet_inference_time/custom_inference_time:.1f}x slower\")\n",
    "\n",
    "print(\"\\n3. SUITABILITY FOR EDGE DEVICES AND REAL-TIME APPLICATIONS:\")\n",
    "print(\"   Custom CNN:\")\n",
    "print(\"     ✓ Lightweight and fast\")\n",
    "print(\"     ✓ Low memory footprint\")\n",
    "print(\"     ✓ Excellent for edge devices\")\n",
    "print(\"     ✓ Real-time capable\")\n",
    "print(\"     ✗ Lower accuracy\")\n",
    "\n",
    "print(\"   MobileNetV2:\")\n",
    "print(\"     ✓ Higher accuracy\")\n",
    "print(\"     ✓ Pretrained features\")\n",
    "print(\"     ✓ Still relatively efficient (designed for mobile)\")\n",
    "print(\"     ⚠ Larger model size\")\n",
    "print(\"     ⚠ Higher computational requirements\")\n",
    "print(\"     ⚠ May require optimization for real-time edge deployment\")\n",
    "\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "print(\"- Use Custom CNN for: Resource-constrained environments, real-time applications\")\n",
    "print(\"- Use MobileNetV2 for: Applications where accuracy is critical, sufficient computational resources available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_pytorch_env]",
   "language": "python",
   "name": "conda-env-my_pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
